{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center\"> Introduction to Python\n",
    "\n",
    "## <div style=\"text-align: center\">Machine Learning in Python (VI) - Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](data:image/jpeg;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Image analysis\n",
    "\n",
    "Neural networks are very often used for image or sound analysis. Normal sequential networks with connections all-to-all are used for:\n",
    "- convolutional neural networks, **CNN**, which are used for image analysis\n",
    "- recurrent neural networks, **RNN**, which are used for sound analysis\n",
    "\n",
    "Most often the fun with convolutional networks and image analysis starts with recognizing numbers. However, we will do something a little more interesting / funny - we will play with recognizing whether there is a dog or cat in the picture.  \n",
    "You will find the data you need for this here: https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition  \n",
    "<br><br>\n",
    "On the `data` tab of this page there are two downloadable `train.zip` and `test.zip` files. Just download the `train.zip` and then extract to the folder where this notebook is located to the `cats_vs_dogs` folder, the subfolder `train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the names of image files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog.8011.jpg', 'cat.5077.jpg', 'dog.7322.jpg']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "img_dir = r'/Users/kubamilcarz/My Drive/College/Sophomore/Python/Project/train'\n",
    "images = os.listdir(img_dir)\n",
    "images[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_images = [os.path.join(img_dir, img) for img in images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/kubamilcarz/My Drive/College/Sophomore/Python/Project/train/dog.8011.jpg',\n",
       " '/Users/kubamilcarz/My Drive/College/Sophomore/Python/Project/train/cat.5077.jpg',\n",
       " '/Users/kubamilcarz/My Drive/College/Sophomore/Python/Project/train/dog.7322.jpg']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_images[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a function for showing rgb layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_rgb_layers(image, subplots_args=dict()):\n",
    "    '''\n",
    "    Show RGB layers of the image on separate axes.\n",
    "    '''\n",
    "    \n",
    "    im_shape = image.shape\n",
    "    \n",
    "#     check if the loaded image has 3 dimensions\n",
    "    assert image.ndim == 3\n",
    "#     check whether the last dimension of im_shape has 3 dimensions\n",
    "    assert im_shape[-1] == 3\n",
    "    \n",
    "#     plot rgb layers\n",
    "    fig, ax = plt.subplots(ncols=3)\n",
    "    for idx, layer in enumerate(['Reds', 'Greens', 'Blues']):\n",
    "        ax[idx].imshow(image[:, :, idx], cmap=layer)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We're loading images\n",
    "We load 10000 images, 5000 each per category (training on all would take too long)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "**************************************************\n",
      "**************************************************\n",
      "**************************************************\n",
      "**************************************************\n",
      "**************************************************\n",
      "**************************************************\n",
      "**************************************************\n",
      "**************************************************\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "from imports_for_ML import load_images\n",
    "\n",
    "X, y = load_images(img_dir, n_images=10000, resize=(50, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 50, 50, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 16:04:48.690940: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "\n",
    "# we create two convolution layers, then max pooling\n",
    "\n",
    "    Conv2D(16,(3,3), activation = 'relu' ), #16 filters/neurons, 3 by 3\n",
    "    Conv2D(16,(3,3), activation = 'relu' ),\n",
    "    MaxPooling2D((3,3)),\n",
    "# we create next two convolution layers, then max pooling\n",
    "    Conv2D(32,(3,3), activation = 'relu' ),\n",
    "    Conv2D(32,(3,3), activation = 'relu' ),\n",
    "    MaxPooling2D((2,2)), #for dimensionality reduction - here 2 by 2 convey even more dense information. \n",
    "    #we need double brackers because size is one argument \n",
    "\n",
    "# finally, we flatten all the filters to one vector and then add 64 ordinary neurons\n",
    "    Flatten( input_shape = (5, 5,32)),\n",
    "    Dense(64, activation = 'relu'),\n",
    "\n",
    "# and the output neuron that says dog (1) or cat (0)\n",
    "    Dense(1, activation='sigmoid')\n",
    "\n",
    "# and compile, like always before\n",
    "])\n",
    "model.compile(loss = 'binary_crossentropy', optimizer  = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "282/282 [==============================] - 6s 20ms/step - loss: 0.8857 - accuracy: 0.5438\n",
      "Epoch 2/10\n",
      "282/282 [==============================] - 5s 19ms/step - loss: 0.6461 - accuracy: 0.6186\n",
      "Epoch 3/10\n",
      "282/282 [==============================] - 6s 20ms/step - loss: 0.6089 - accuracy: 0.6679\n",
      "Epoch 4/10\n",
      "282/282 [==============================] - 6s 21ms/step - loss: 0.5665 - accuracy: 0.7100\n",
      "Epoch 5/10\n",
      "282/282 [==============================] - 5s 19ms/step - loss: 0.5229 - accuracy: 0.7459\n",
      "Epoch 6/10\n",
      "282/282 [==============================] - 5s 19ms/step - loss: 0.5035 - accuracy: 0.7549\n",
      "Epoch 7/10\n",
      "282/282 [==============================] - 6s 20ms/step - loss: 0.4590 - accuracy: 0.7812\n",
      "Epoch 8/10\n",
      "282/282 [==============================] - 6s 22ms/step - loss: 0.4323 - accuracy: 0.8008\n",
      "Epoch 9/10\n",
      "282/282 [==============================] - 5s 19ms/step - loss: 0.3916 - accuracy: 0.8260\n",
      "Epoch 10/10\n",
      "282/282 [==============================] - 5s 19ms/step - loss: 0.3541 - accuracy: 0.8403\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 48, 48, 16)        448       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 46, 46, 16)        2320      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 15, 15, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 13, 13, 32)        4640      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 11, 11, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 800)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                51264     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 67,985\n",
      "Trainable params: 67,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the training data:\n",
      "282/282 [==============================] - 2s 8ms/step - loss: 0.2890 - accuracy: 0.8829\n",
      "0.882888913154602\n",
      "\n",
      "Accuracy on the test data:\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.5768 - accuracy: 0.7540\n",
      "0.7540000081062317\n"
     ]
    }
   ],
   "source": [
    "# plt.plot(history.history['acc'])\n",
    "# plt.ylabel('Accuracy')\n",
    "\n",
    "print('Accuracy on the training data:')\n",
    "print(model.evaluate(X_train, y_train)[1])\n",
    "\n",
    "print('\\nAccuracy on the test data:')\n",
    "print(model.evaluate(X_test, y_test)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fce694bb820>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionDeniedError",
     "evalue": "/Users/akovbasiuk; Permission denied",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionDeniedError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Users/akovbasiuk/Documents/Documents_Anna_MacBook_Pro/Classes/Python/class 10/cats_dogs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_format\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mh5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/networks/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/networks/lib/python3.9/site-packages/tensorflow/python/lib/io/file_io.py:511\u001b[0m, in \u001b[0;36mrecursive_create_dir_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mio.gfile.makedirs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecursive_create_dir_v2\u001b[39m(path):\n\u001b[1;32m    501\u001b[0m   \u001b[38;5;124;03m\"\"\"Creates a directory and all parent/intermediate directories.\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \n\u001b[1;32m    503\u001b[0m \u001b[38;5;124;03m  It succeeds if path already exists and is writable.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;124;03m    errors.OpError: If the operation fails.\u001b[39;00m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 511\u001b[0m   \u001b[43m_pywrap_file_io\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRecursivelyCreateDir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_to_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mPermissionDeniedError\u001b[0m: /Users/akovbasiuk; Permission denied"
     ]
    }
   ],
   "source": [
    "keras.models.save_model(model = model, filepath = '/Users/akovbasiuk/Documents/Documents_Anna_MacBook_Pro/Classes/Python/class 10/cats_dogs', save_format ='h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here how to use your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-25 12:22:13.123769: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = load_model((r'/Users/akovbasiuk/Documents/Documents_Anna_MacBook_Pro/Classes/Python/class 10/cats_dogs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 48, 48, 16)        448       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 46, 46, 16)        2320      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 15, 15, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 13, 13, 32)        4640      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 11, 11, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 800)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                51264     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 67,985\n",
      "Trainable params: 67,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correctness of the test data is significantly higher than 50%, so the model has learned something. In the case above we also see that the correctness on the test data is not very different from the correctness on the training data.  \n",
    "Therefore we will try to train the network further. To make the work easier this time we will separate 4% of the training data (i.e. 0.04 * 9000, i.e. 360 pictures) into an independent validation set - on this set of pictures we do not teach the network, but use it on an ongoing basis to monitor the correctness of the network on independent data during the training. When the correctness of this independent data stops improving - the trening will be stopped.\n",
    "* To be able to stop the trening earlier, we import `EarlyStopping` and create a `stop_early` by setting the `patience` from `EarlyStopping` to `2` (i.e., the trening will be stopped if we don't get better validation data for two consecutive epochs.\n",
    "* Then set the `validation_split` in the `model.fit` to the appropriate value.\n",
    "* Add stop_early as a callback, set the callbacks argument in the model.fit. `callbacks` always takes the list, so if you only give one callback, make sure to put it in the list (i.e., instead of `some_callback` you give `[some_callback]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "⚠️TASK 1 (1 min)\n",
    "<br>\n",
    "\n",
    "Using the information above, please finish the code below:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "stop_early = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(                                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training was interrupted because the correctness on the validation data stopped growing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "⚠️TASK 2 (2 min)\n",
    "<br>\n",
    "\n",
    "Let's see how our model works now. First, as before, we will evaluate the correctness on the test and training data:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that further training has improved the model on training data, but not on test data.\n",
    "This is a symptom that the network has started to overfit - later on we will try to prevent this by adding a regulation to the network.  \n",
    "Before we move on to this step, let's check a few examples that the network recognizes correctly, and a few that are not recognized correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take the first 25 test photos:\n",
    "test_sample = X_test[:25]\n",
    "predictions = model.predict(test_sample)\n",
    "true_labels = y_test[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_idx = 9\n",
    "plt.imshow(test_sample[check_idx] / 255)\n",
    "\n",
    "\n",
    "text = 'The network is: {:.2f}% sure it is a dog,\\nbut really it is a {}, so the network {}'\n",
    "correct_response = true_labels[check_idx]\n",
    "it_is_really = ['cat', 'dog'][correct_response]\n",
    "net_claims = predictions[check_idx, 0]\n",
    "is_net_correct = np.abs(net_claims - correct_response) < 0.5\n",
    "\n",
    "plt.title(text.format(net_claims * 100., it_is_really, ['is wrong', 'is right'][is_net_correct]))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What has our network learned?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now visualize some first layer filters that the network has learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's display the filter weights of the first network layer. Our neural network, present in the `model` variable, has a `layers` method, where we can find all network layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "⚠️TASK 3 (3 min)\n",
    "<br>\n",
    "\n",
    "To find the weights of the first layer filters we have to:\n",
    "* select this layer from the list (usual indexing)\n",
    "* use the `get_weights()` method on the result (without arguments)\n",
    "* `get_weights` returns a list of two elements: filter weights, and the resting value of the filters. Therefore, we will select the first element - the filter weights.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights should be `3 x 3 x 3 x 16` (*Height* x *Width* x *Depth (RGB)* x *Number of filters*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=4, ncols=4)\n",
    "ax = ax.ravel()\n",
    "\n",
    "for idx in range(len(ax)):\n",
    "    # you have to give a dimension that we average, remember that single\n",
    "    # filter has dimensions: height to width on RGB\n",
    "    average_weights = weights[..., idx].mean(axis=2)\n",
    "    ax[idx].imshow(average_weights)\n",
    "    ax[idx].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network regularization\n",
    "We will now try to improve network performance through regularization. We are building the whole network as before, but we add:\n",
    "* `Dropout` to one of the last layers.\n",
    "* `BatchNormalization` between convoluted layers (`Conv2d`)  \n",
    "\n",
    "Dropout is a simple way of regularization - from time to time some neurons are randomly removed from a given layer. This leads to much slower learning, but prevents neurons or combinations of neurons from remembering specific training examples (and, therefore, overfitting!).  \n",
    "`Dropout(0.25)` leads to the ejection of 25% of the neurons at each training stage. In our case, we want to discard 50% of the neurons during training.  \n",
    "`BatchNormalization` is a method of normalizing data going from one layer to the next, which usually improves the learning of the network, in this case by giving `BatchNormalization()` to `model.add()` (normalization is something like standardization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "⚠️TASK 4 (6 min)\n",
    "<br>\n",
    "\n",
    "Import Dropout and BatchNormalization from `keras.layers`, and then create the modified network.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization leads to a much slower learning pace, so this time we will be training for more epochs - at least 20. We also use, as before, 4% of the data for validation and we want the training to end if there is no improvement for four epochs (this time four - because training will be much more difficult for the network with regularization).  \n",
    "The training will take some time, so patience..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "⚠️TASK 5 (2 min)\n",
    "<br>\n",
    "\n",
    "Fill out the code below and run the cells.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = \n",
    "history = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that with the successive epochs the network correctness increases linearly for training data, while for validation data we observe significant variability with a tendency to improve correctness (and cost, i.e. `val_loss`). However, the network does not achieve a higher validation rate than 84% on validation data (we would have to build a bigger network for this). The training ends before the end of 20 epochs because we no longer see any improvement. We can now check the correctness on the test data. But before that - the chart of changes in correctness on training and validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='training')\n",
    "plt.plot(history.history['val_accuracy'], label='validation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corr = model.fit(X_train, y_train)\n",
    "\n",
    "test_corr = model.fit(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using reguralization, we come to the correctness slightly above 78%, almost 80%. This is a significant improvement, but we are still far from human correctness. However, we have seen that the networks do not understand the picture as we do - a picture that is an incomprehensible noise can convince the net that it has to do with a dog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is still not a model that we would be satisfied with in practice, but for playing on a weak computer it is ok. To have better accuracy, we would also need more data - i.e. all 25,000 photos to be loaded. We could also increase the scaling (currently we are scaling all the photos to 50 by 50 pixels - in this size some photos will be difficult to identify.  \n",
    "Also, with `25,000` photos you actually produce up to half a million photos - by rotating, scaling and cropping the photos (this can be done automatically using `keras.preprocessing.image.ImageDataGenerator`. This number of examples would lead our network to have much better accuracy.  <br><br>\n",
    "Another strategy is to use a network previously trained on millions of photos (there are a lot of such public domain architectures - including some very well known architectures) and stick several hundred neurons to its top layers to classify dog vs. cat. Then the training would take place on such a hybrid network - a large part of it would be frozen, already trained before, and only the final layer (or layers) are trained using the available data. This kind of strategy is called *transfer learning*, it is the last of our struggle with image analysis. We won't take full advantage of *transfer learning* here - and we'll only see how helpful the networks already trained on millions of images are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning\n",
    "We will use the ResNet50 network, which together with the weights can be downloaded from the web using the Keras library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "\n",
    "resnet_model = ResNet50()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my case, the network weights are not downloaded because I downloaded them earlier. It may take 1-8 minutes (depending on the connection speed) to download network scales.  \n",
    "The `ResNet50` network is huge, compared to the several layers of network we trained, let's see a summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(resnet_model.layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many parameters does the network have in total (you will find this information at the bottom of the description)? Hint: the number of parameters here is already counted in millions. Such networks are trained on many cores and many GPUs for days (and sometimes weeks) on millions of photos.\n",
    "The network is trained on the basis of `ImageNet`, which has 14 million images, and the task of the network is to identify 1000 different types of objects (including people, cars, animals, food, etc.). Among the animals, the network must not only distinguish between cats and dogs, but also distinguish between several species of dogs and cats. A network trained on this basis is therefore ideal for our task.  \n",
    "However, before we use it, we have to process our photos differently - the `ImageNet` database consists of photos with a different resolution (`224 x 224`). In addition, these images are processed a little differently for `ResNet`. So we have to load the data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size_MB = X.size * X.itemsize / 1000000\n",
    "print('The data has now: {} MB'.format(data_size_MB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_images(img_dir, n_images=10000, resize=(224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have problems with RAM - you can load accordingly less photos above (for example 5000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size_MB = X.size * X.itemsize / 1000000\n",
    "print('The data has now: {} MB'.format(data_size_MB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the network predictions for a few pictures: 1543 and 5623. Let's see these images first:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "⚠️TASK 6 (4 min)\n",
    "<br>\n",
    "\n",
    "Please visualise dog cat and dog picture numbers 1543 and 5623. How can we access correctly the picture numbers? Check the `images` list - the images are not sorted properly!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now process the data to match the ResNet50 format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "X = preprocess_input(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what predictions ResNet gives for these images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = resnet_model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `preds` is now a matrix of two lines and 1000 columns. Rows are predictions for subsequent images, and columns are categories of predictions (ImageNet is a task in which the network classifies images as belonging to one of 1000 categories)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand what it means by which category we will use the `decode_predictions` function, giving it a `preds` matrix. We will also ask this function to give class names for only 5 classes with the highest certainty (for each prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import decode_predictions\n",
    "\n",
    "labels = decode_predictions(preds, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have translated the predictions in the `labels` variable. Let's see what predictions the network gave for the first image (cat):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "75% for an American staffordshire terrier, which is the correct answer. We can see that the network is doing quite well in recognizing the animals we are interested in. How do we use it for our purposes? We have two options:\n",
    "* we use the network the way it works now and collect % confidence inside the dog category and inside the cat category and see if the cat or dog has a higher percentage\n",
    "* we cut off the last layer of the net (1000 neurons corresponding to the categories) and stick one (dog vs. cat), then we train the net on our pictures. However, we do not train the whole network, but only the last layer of the net  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
